{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfdt\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from keras.layers import Dense, MultiHeadAttention, LayerNormalization, Dropout, Input, Embedding, GlobalAveragePooling1D\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "data = pd.read_csv('./dataset/dataset.csv')\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size :  96000\n",
      "Val Size :  12000\n",
      "Train Length : 96000\n",
      "Val Length : 12000\n",
      "Test Length : 12000\n"
     ]
    }
   ],
   "source": [
    "# Split the Dataset\n",
    "\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = int(0.1 * len(data))\n",
    "\n",
    "print('Train Size : ', train_size)\n",
    "print('Val Size : ', val_size)\n",
    "\n",
    "train_df = data[:train_size]\n",
    "val_df = data[train_size:train_size+val_size]\n",
    "test_df = data[train_size+val_size:]\n",
    "\n",
    "print(f'Train Length : {len(train_df)}')\n",
    "print(f'Val Length : {len(val_df)}')\n",
    "print(f'Test Length : {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
      "<MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
      "<MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "text_col = 'description'\n",
    "label_col = 'label'\n",
    "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "def subtract_one_label(text, label):\n",
    "  return text,label -1\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Pake model pre trained jadi langsung jadi \n",
    "\n",
    "# Convert dataframe (training, validation, and testing) ke dalam tensorflow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_df[text_col].values, train_df[label_col].values))\n",
    "train_data = train_data.map(subtract_one_label)\n",
    "print(train_data)\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((val_df[text_col].values, val_df[label_col].values))\n",
    "val_data = val_data.map(subtract_one_label)\n",
    "print(val_data)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_df[text_col].values, test_df[label_col].values))\n",
    "test_data = test_data.map(subtract_one_label)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RandomDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "# Pre Processing\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "def encode_text(text, label):\n",
    "  encoded_text = bert_tokenizer.encode(text.numpy().decode('utf-8', add_special_tokens=True, max_length=MAX_LENGTH, truncation=True, padding='max_length'))\n",
    "  return encoded_text, label\n",
    "\n",
    "train_data = train_data.map(lambda text, label: tf.py_function(encode_text, [text, label], [tf.int64, tf.int64]))\n",
    "val_data = val_data.map(lambda text, label: tf.py_function(encode_text, [text, label], [tf.int64, tf.int64]))\n",
    "test_data = test_data.map(lambda text, label: tf.py_function(encode_text, [text, label], [tf.int64, tf.int64]))\n",
    "\n",
    "# Pad dataset and batch dataset\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1], []))\n",
    "val_data = val_data.padded_batch(BATCH_SIZE, padded_shapes=([-1], []))\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1], []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "  def __init__(self, embedding_dimension, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dimension)\n",
    "    self.ffn = Sequential([\n",
    "      Dense(ff_dim, activation='relu'),\n",
    "      Dense(embedding_dimension)\n",
    "    ])\n",
    "    self.layernormal1 = LayerNormalization(epsilon=0.000001)\n",
    "    self.layernormal2 = LayerNormalization(epsilon=0.000001)\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate) \n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    attention_output = self.att(inputs, inputs)\n",
    "    attention_output = self.dropout1(attention_output, training=training)\n",
    "    out1 = self.layernormal1(inputs + attention_output)\n",
    "    ffn_output = self.ffn(out1)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    return self.layernormal2(out1 + ffn_output)\n",
    "  \n",
    "# Parameter untuk model\n",
    "\n",
    "num_heads = 2\n",
    "embed_dimension = 128\n",
    "ff_dim = 32\n",
    "\n",
    "VOCAB_SIZE = bert_tokenizer.vocab_size\n",
    "input = Input(shape=(MAX_LENGTH,))\n",
    "embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=embed_dimension)(input)\n",
    "transformer_block = TransformerBlock(embed_dimension, num_heads, ff_dim)\n",
    "x = transformer_block(embedding_layer)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(4, activation='softmax')(x) # 4 masukin ke number of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=input, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: decode() takes at most 2 arguments (5 given)\nTraceback (most recent call last):\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\tinti\\AppData\\Local\\Temp\\ipykernel_8528\\2539009883.py\", line 8, in encode_text\n    encoded_text = bert_tokenizer.encode(text.numpy().decode('utf-8', add_special_tokens=True, max_length=MAX_LENGTH, truncation=True, padding='max_length'))\n\nTypeError: decode() takes at most 2 arguments (5 given)\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  TypeError: decode() takes at most 2 arguments (5 given)\nTraceback (most recent call last):\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\tinti\\AppData\\Local\\Temp\\ipykernel_8528\\2539009883.py\", line 8, in encode_text\n    encoded_text = bert_tokenizer.encode(text.numpy().decode('utf-8', add_special_tokens=True, max_length=MAX_LENGTH, truncation=True, padding='max_length'))\n\nTypeError: decode() takes at most 2 arguments (5 given)\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_11415]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mf:\\github\\post-training-deep-learning\\Subco\\Session 04. Word Embeddings\\word-embedding.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github/post-training-deep-learning/Subco/Session%2004.%20Word%20Embeddings/word-embedding.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Training Model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github/post-training-deep-learning/Subco/Session%2004.%20Word%20Embeddings/word-embedding.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print(train_data)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/github/post-training-deep-learning/Subco/Session%2004.%20Word%20Embeddings/word-embedding.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(val_data)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/github/post-training-deep-learning/Subco/Session%2004.%20Word%20Embeddings/word-embedding.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_data, validation_data\u001b[39m=\u001b[39;49mval_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\tinti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: decode() takes at most 2 arguments (5 given)\nTraceback (most recent call last):\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\tinti\\AppData\\Local\\Temp\\ipykernel_8528\\2539009883.py\", line 8, in encode_text\n    encoded_text = bert_tokenizer.encode(text.numpy().decode('utf-8', add_special_tokens=True, max_length=MAX_LENGTH, truncation=True, padding='max_length'))\n\nTypeError: decode() takes at most 2 arguments (5 given)\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  TypeError: decode() takes at most 2 arguments (5 given)\nTraceback (most recent call last):\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\tinti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\tinti\\AppData\\Local\\Temp\\ipykernel_8528\\2539009883.py\", line 8, in encode_text\n    encoded_text = bert_tokenizer.encode(text.numpy().decode('utf-8', add_special_tokens=True, max_length=MAX_LENGTH, truncation=True, padding='max_length'))\n\nTypeError: decode() takes at most 2 arguments (5 given)\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_11415]"
     ]
    }
   ],
   "source": [
    "# Training Model\n",
    "# print(train_data)\n",
    "# print(val_data)\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f'Accuracy : {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted\n",
    "def predict(model, sentence, tokenizer, max_length, class_names):\n",
    "  sample = tokenizer.encode(sentence[0], add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length')\n",
    "  sample = np.array([sample]) # Convert sample to 2d Array\n",
    "  predictions = model.predict(sample)\n",
    "  print('Predictions : ', predictions)\n",
    "  predicted_class = np.argmax(predictions[0])\n",
    "  print(f'Predicted Class : {predicted_class}')\n",
    "  print(f'Predicted Class Name : {class_names[predicted_class]}')\n",
    "\n",
    "sample = ['the Red Sox last night appeared poised to move on without him as Curtis Leskanic inched closer to rejoining the club. The Sox initially indicated they would make an announcement last night on Williamson\\'s injury but changed their position during the first inning of the game ...']\n",
    "predict(model, sample, bert_tokenizer, MAX_LENGTH, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(history.history.keys()))\n",
    "\n",
    "# Sub plot to see models quality\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], 'o-', label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], 'x-', label=\"Validation Accuracy\")\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], 'o-', label ='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'x-', label ='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Semakin turun angkanya semakin bagus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
